# ██████╗  ██████╗ █████╗
# ██╔══██╗██╔════╝██╔══██╗
# ██║  ██║██║     ███████║
# ██║  ██║██║     ██╔══██║
# ██████╔╝╚██████╗██║  ██║
# ╚═════╝  ╚═════╝╚═╝  ╚═╝
# DEPARTAMENTO DE ENGENHARIA DE COMPUTACAO E AUTOMACAO
# UNIVERSIDADE FEDERAL DO RIO GRANDE DO NORTE, NATAL/RN
#
# (C) 2023 CARLOS M D VIEGAS
# https://github.com/cmdviegas

### Description:
# This is a docker-compose file that creates a stack of nodes running Apache Hadoop 3.3.5 and Spark 3.4.0. Optionally, it includes Apache Hive 3.1.3 with Postgresql 15.2 as metastore.

### How it works:
# It initializes 'node-master' container with hadoop and spark services and multiple 'node-X' containers as worker nodes (according to the number of replicas). 'node-master' starts hadoop and spark services and then creates a cluster by connecting to each 'node-X'. There is an .env file that defines some environment variables that should be edited by the user.

version: '3.9'

services:
  node-master:
    container_name: node-master
    hostname: node-master
    image: hadoopcluster/${IMAGE_VER}
    build:
      context: .
      dockerfile: Dockerfile
      args:
        USER: ${SYS_USERNAME} # USERNAME (change at .env file)
        PASS: ${SYS_PASSWORD} # USER PASSWORD (change at .env file)
    tty: true
    restart: no
    networks:
      hadoop_network:
        ipv4_address: ${IP_NODEMASTER}
    ports:
      - "9870:9870/tcp" # HDFS
      - "8088:8088/tcp" # YARN
      - "4040:4040/tcp" # SPARK HISTORY (DURING A VALID SPARKSESSION)
      - "18080:18080/tcp" # SPARK HISTORY SERVER
      - "27017:27017/tcp"
      - "2222:22/tcp" # SSH
    volumes:
      - ./apps:/home/${SYS_USERNAME}/apps
      - .env:/home/${SYS_USERNAME}/.env
      - node-master:/home/${SYS_USERNAME}/
    entrypoint: ./bootstrap.sh
    command: HADOOP
    healthcheck:
      test: bash -c 'ssh -q -o ConnectTimeout=1 ${SYS_USERNAME}@node-master exit'
      start_period: 3s
      interval: 2s
      timeout: 3s
      retries: 3

  node:
    image: hadoopcluster/${IMAGE_VER}
    deploy:
      mode: replicated
      replicas: ${NODE_REPLICAS}
    tty: true
    restart: on-failure:2
    ports:
      - "8042-8062:8042/tcp" # YARN
    networks:
      - hadoop_network
    depends_on:
      node-master:
        condition: service_healthy
    entrypoint: ./bootstrap.sh
    command: HADOOP
    volumes:
      - ./apps:/home/${SYS_USERNAME}/apps
      - .env:/home/${SYS_USERNAME}/.env
      - /home/${SYS_USERNAME}/

  hive:
    container_name: hive
    hostname: hive-server
    image: hadoopcluster/${IMAGE_VER}
    tty: true
    restart: unless-stopped
    ports:
      - "10000:10000/tcp" # HIVE SERVER
      - "10002:10002/tcp" # HIVE WEB INTERFACE
    networks:
      hadoop_network:
        ipv4_address: ${IP_HIVE}
    profiles:
      - hive
    depends_on:
      node-master:
        condition: service_healthy
    entrypoint: ./bootstrap.sh
    command: HIVE
    volumes:
      - ./apps:/home/${SYS_USERNAME}/apps
      - .env:/home/${SYS_USERNAME}/.env
      - /home/${SYS_USERNAME}/

  mongo:
    image: mongo
    restart: always
    container_name: mongo
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example
    #      MONGO_INITDB_DATABASE: init
    volumes:
      - mongo_data:/data/db
      - /etc/timezone:/etc/timezone:ro

  mongo-express:
    image: mongo-express
    restart: always
    container_name: mongo_ui
    ports:
      - 8081:8081
    environment:
      ME_CONFIG_MONGODB_ADMINUSERNAME: root
      ME_CONFIG_MONGODB_ADMINPASSWORD: example
      ME_CONFIG_MONGODB_URL: mongodb://root:example@mongo:27017/

networks:
  hadoop_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: ${IP_RANGE}

volumes:
  node-master:
  mongo_data:
